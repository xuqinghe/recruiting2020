参考文献：https://mp.weixin.qq.com/s/6sTA_DLijFgYHLI7O6uvgg

目标检测算法分为两类：

一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN，Faster R-CNN），two-stage，需要先产生目标候选框，再对候选框做分类与回归

另一类是one-stage的YOLO、SSD，仅仅使用一个卷积神经网络CNN直接预测不同目标的类别与位置

区别：第一类方法的准确度高一些，但是速度慢，第二类速度快，但是准确性要低一些。

1、R-CNN 滑动窗口思想

输入一张图片，通过指定算法从图片中提取2000个类别独立的候选区域，对每个候选区域利用卷积神经网络来获取一个特征向量，对与每个区域相应的特征向量，利用SVM进行分类，并通过一个bounding box regression调整目标包围框的大小。

提取候选区域：objectness；selective search
提取特征向量：AlexNet2012 输入是227*227
	有监督的预训练：训练网络参数 ImageNet 
	特定样本下的微调：训练网络参数 

SVM分类：输入特征向量，输出类别得分
	ImageNet数据
	2000*20 = 2000*4096 * 4096*20
	非极大值抑制提出重叠建议框
	
	SVM训练：二分类，需要对每一个类别训练单独的SVM
	由于负样本太懂，采用hard negative mining

边框修正：输入为pool5层的4096维特征向量，输出为x、y方向的缩放和平移


主要贡献：1、用CNN提取特征；2、使用bbox regression进行目标包围框的修正
主要问题：1、耗时的selective search 一张图2s ；2、耗时的串行CNN前向传播，每一个候选框经过AlexNet提取特征，所有框耗时47s；3、三个模块（CNN特征提取、SVM分类和边框修正）是分别训练的，并且在训练的时候，对于存储空间的消耗很大。

2、Fast R-CNN

首先还是采用selective search提取2000个候选框RoI；
使用一个卷积神经网络对全图进行特征提取；
使用一个RoI Pooling Layer在全图特征上摘取每一个RoI对应的特征；
分别经过为21和84维的全连接层（并列的，前者为分类输出，后者为回归输出）Fast R-CNN通过CNN直接获取整张图像的特征图，在使用RoI Pooling Layer在特征图上获取对应每个候选框的特征，避免了R-CNN中的每个候选框串行卷积（耗时较长）。

RoI Pooling Layer：对于每个RoI而言，需要从共享卷积层获取的特征图上提取对应的特征，丙炔送入全连接层进行分类。因此，RoI Pooling主要做了两件事，第一件是为每个RoI选取对应的特征，第二件事是为了满足全连接层的输入需求，将每个RoI对应的特征的维度转化为某个定值。

将region proposal划分为目标H*W大小的分块
对每一个分块中做MaxPooling
将所有输出值组合起来便形成固定大小为H*W的feature map

主要贡献：1、取代R-CNN串行特征提取方式，直接采用一个CNN对全图提取特征；2、除了selective search，其他部分都可以合在一起训练。
主要缺点：耗时的selective search还存在

3、Faster R-CNN

取代了selective search，直接通过一个Region Proposal Network（RPN）生成待检测区域，这么做，在生成RoI区域的时候，时间就从2s缩减到了10ms。

Faster R-CNN由共享卷积层、RPN、RoI pooling以及分类和回归四部分组成：1、首先使用共享卷积层为全图提取特征feature maps；2、将得到的feature maps送入RPN，RPN生成待检测框（指定RoI的位置），并对RoI的包围框进行第一次修正；3、RoI Pooling Layer根据RPN的输出在feature map上面选取每个RoI对应的特征，并将维度置为定值；4、使用全连接层（FC Layer）对框进行分类，并且进行目标包围框的第二次修正。

尤其注意的是，Faster R-CNN真正实现了端到端的训练（end-to-end training），Faster R-CNN最大特色是使用了RPN取代了SS算法来获取RoI

RPN：网络分为2条支线，上面一条支线通过softmax来分类anchors获得前景和背景，下面一条支线用于计算anchors的边框偏移量，以获得精确的proposals，而最后的proposal层则负责综合foreground anchors和偏移量获取proposals，同时剔除太小和超出边界的proposals。

anchor：简单的说，RPN依靠一个在共享特征图上滑动的窗口，为每个位置生成9种预先设置好长宽比与面积的目标框（即anchor），面积128*128,256*256,512*512 每种面积的长宽比为1:1,1:2,2:1

分类和定位


4、Mask R-CNN 目标分类、目标检测、语义分割、实例分割、人体姿势识别

分为3个模块：Faster R-CNN 、 RoI Align 和 Mask



5、YOLO 45FPS

分为三个部分：卷积层、目标检测层、NMS筛选层

卷积层：采用Google inception V1网络，进行了改造，没有使用inception module，而是用一个1*1的卷积，并联一个3*3的卷积来代替，简化网络结构。这一层主要是进行特征提取，从而提高模型泛化能力。

目标检测层：4个卷积层（为了提高模型泛化能力）和2个全连接层，最后生成7*7*30的输出。yolo将448*448的原图分割成了7*7个网格，然后每个单元格负责去检测那些中心点落在该格子内的目标。

NMS筛选层：为了在多个结果中筛选出最合适的几个。先过滤掉score低于阈值的box，对剩下的box进行NMS非极大值抑制，去除掉重叠度比较高的box。这样就得到了最终的最合适的几个box和他们的类别。

Yolo损失函数：位置误差（mse），confidence误差，分类误差（mse，交叉熵）。

Yolo算法开创了one-stage检测的先河，他将物体分类和物体检测网络合二为一，都在全连接层完成，故它大大降低了目标检测的耗时，提高了实时性。

缺点：1、每个网格只对应两个bounding box，当物体的长宽比不常见，效果差；2、原始图片只划分了7*7的网格，当两个物体靠的很近时，效果很差；3、最终每个网格只对应一个类别，容易出现漏检；4、对于图片中比较小的物体，效果很差。这其实是所有目标检测算法的通病，SS对它有些优化。

6、SSD

Faster R-CNN准确率mAP较高，漏检率recall较低，但速度较慢。而Yolo则相反，速度快，但准确率和漏检率不尽人意。SSD综合了他们的优缺点，对输入300*300的图像，在VOC2007数据集上test，能够达到58FPS，72.1%的mAP.

SSD和YOLO一样都是采用一个CNN网络来进行检测，但是却采用了多尺度的特征图。

和YOLO一样，也分为三部分：卷积层，目标检测层和NMS筛选层。

卷积层：SSD论文采用了VGG16的基础网络，其实这也是几乎所有检测神经网络的惯用方法。先用一个CNN网络来提取特征，然后再进行后续的目标定位和目标分类识别。

目标检测层：5个卷积层和1个平均池化层组成，去掉了最后的全连接层。SSD人为目标检测中的物体，只与周围信息相关，它的感受野不是全局的，故没必要也不应该做全连接。

SSD的特点：1、多尺度feature map上进行目标检测：每一个卷积层都会输出不同大小感受野的feature map，在这些feature map上进行目标位置和类别的训练和预测，从而达到多尺度检测的目的，可以克服yolo对于宽高比不常见的物体，识别准确率较低的问题。而yolo中，只在最后一个卷积层上做目标位置和类别的训练和预测。这是SSD相对于yolo能提高准确率的一个关键所在。2、设置先验框：在yolo中每个单元预测多个边界框，SSD和Faster R-CNN相似，也提出了anchor的概念，卷积输出的feature map，每个点对应为原图的一个区域的中心点，以这个点为中心，构造出6个宽高比例不同，大小不同的anchor，每个anchor对应4个位置参数（w,y,w,h)和21个类别概论。3、SSD采用数据增强：生成与目标物体真实box间IOU为0.1、0.3、0.5、0.7、0.9的patch，随机选取这些patch参与训练，并对他们进行随机水平翻转等操作。SSD认为这个策略提高了8.8%的准确率。

筛选层：SSD综合了各个不同feature map上的目标检测输出default box

7、YOLOv2

针对YOLO准确率不高，容易漏检，对长宽比不常见物体效果差等问题，结合SSD的特点，提出了YOLOv2，它主要还是采用了YOLO的网络结构，在其基础上做了一些优化和改进。

优化和改进：1、网络采用DarkNet-19：19层，里面包含了大量3*3卷积，同时借鉴inceptionV1加入1*1卷积核全局平均池化层；2、去掉全连接层：和SSD一样，模型中只包含卷积层和平均池化层（平均池化是为了变为一维向量，做softmax分类），一是物体检测中的目标只是图片中的一个区块，它是局部感受野，没必要做全连接，二是为了输入不同尺寸的图片，如果是全连接，只能输入固定尺寸；3、batch normalization：卷积层加入BN，对下一次卷积输入的数据做归一化，可以在增大学习率的前提下，同样稳定落入局部最优解，从而加速训练收敛，在相同耗时下，增大了有效迭代次数；4、使用anchors：借鉴faster R-CNN和SSD，对于一个中心点，使用多个anchor，得到多个bounding box，每个bounding box包含（x,y,w,h）和21个类别概率信息；5、pass through layer：yolo原本特征图为13*13*256，YOLOv2还利用了之前26*26的特征图进行目标检测，26*26*256的feature map分别按行和列隔点采样，得到4副13*13*256的feature map，将他们组织成一幅13*13*2048的feature map，这样做的目的是提高小物体的识别率，因为越靠前的卷积，其感受野越小，越有利于小物体的识别；6、高分辨率输入Training：YOLO采用224*224图片进行预训练，而YOLOv2则采用448*448；7、Multi-Scale Training：输入不同尺寸的图片，迭代10次，就改变输入图片尺寸。因为去掉了全连接层，故可以输入不同尺寸的图片了。从320*320到608*608。

YOLO和YOLOv2只能识别20类物体，为了优化这个问题，提出了YOLO9000，可以认识9000类物体，它在YOLOv2基础上，进行了imageNet和coco的联合训练，这种方式充分利用imageNet可以识别1000类物体和coco可以进行目标位置检测的优点。当使用imageNet训练时，只更新物体分类的相关参数。而使用coco时，则更新全部所有参数。
























	

